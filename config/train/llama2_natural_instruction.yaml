#model
model_name_or_path: "/home/zx/public/dataset/huggingface/meta-llama/Llama-2-7b-hf" #
cache_dir: null

#dataset
dataset_name: "natural_instruction" #
template: "alpaca"
seed: 2023
dpo_beta: 0.1
dataset_sample: 20000
local_data_dir: "/home/zx/nas/GitRepos/BackdoorFIT/data/natural-instructions" #
na_tasks_file: "" #
label_space_map_file: "/home/zx/nas/GitRepos/BackdoorFIT/config/natural_instruct/polarity/task_sentiment_polarity_label_space.json"

#training
log_with: "none"
learning_rate: 5e-5 #
batch_size: 4 #16
seq_length: 512

epoch_or_step: "step" #epoch
gradient_accumulation_steps: 1
use_auth_token: false
num_train_epochs: 1
max_steps: 10



#eval
eval_batch_size: 4 #16
eval_method: logit #generate

#quantization
load_in_8bit: True
load_in_4bit: false

#peft tuning
use_peft: True
trust_remote_code: false
peft_lora_r: 32
peft_lora_alpha: 64
peft_target_modules: default #all

#log
output_dir: "output"
logging_steps: 100
save_steps: 1000
save_total_limit: 10
push_to_hub: false
hub_model_id: null
gradient_checkpointing: true

#tes
debug: false